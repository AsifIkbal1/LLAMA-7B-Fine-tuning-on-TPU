{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip3 install transformers -q\n!pip3 install datasets evaluate scikit-learn sentencepiece langchain torch_xla[tpuvm] -q\n!pip uninstall tensorflow -y #that's the meme part","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-10-11T00:19:56.247589Z","iopub.execute_input":"2023-10-11T00:19:56.248285Z","iopub.status.idle":"2023-10-11T00:21:09.839354Z","shell.execute_reply.started":"2023-10-11T00:19:56.248255Z","shell.execute_reply":"2023-10-11T00:21:09.838211Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\nimport pandas as pd\nimport numpy as np\nimport datasets\nimport torch\nimport torch_xla.core.xla_model as xm\nimport torch_xla.distributed.xla_multiprocessing as xmp\nimport torch_xla.distributed.parallel_loader as pl\nfrom langchain.prompts import PromptTemplate\nfrom datasets import Dataset\nfrom dataclasses import dataclass\nfrom transformers import AutoTokenizer, AutoModelForCausalLM, DataCollatorWithPadding\nfrom transformers.tokenization_utils_base import PreTrainedTokenizerBase, PaddingStrategy\nfrom tqdm import tqdm\nfrom transformers import logging as hf_logging\n\nos.environ.pop('TPU_PROCESS_ADDRESSES')\nos.environ.pop('CLOUD_TPU_TASK_ID')\nhf_logging.set_verbosity_error()\n\nMAX_INPUT=128\nMODEL = \"/kaggle/input/llama2-7b-hf/Llama2-7b-hf\"","metadata":{"execution":{"iopub.status.busy":"2023-10-11T00:21:09.841185Z","iopub.execute_input":"2023-10-11T00:21:09.841452Z","iopub.status.idle":"2023-10-11T00:21:22.444277Z","shell.execute_reply.started":"2023-10-11T00:21:09.841428Z","shell.execute_reply":"2023-10-11T00:21:22.443112Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"template = \"\"\"Below is an instruction that describes a task. Write a response that appropriately completes the request.\n\n### Instruction: You will be given question with 5 possible answers. Answer the following multiple choice question by giving the most appropriate response. Answer should be one among [A, B, C, D, E]\n\n### Context: {context}\\n\n\n### Question: {prompt}\\n\nA) {a}\\n\nB) {b}\\n\nC) {c}\\n\nD) {d}\\n\nE) {e}\\n\n\n### Response:\n\"\"\"#at first testing I provied answers to this template which resulted in data leak lol\n\nprompt = PromptTemplate(template=template, input_variables=['prompt', 'a', 'b', 'c', 'd', 'e', 'context'])\ntrain_df = pd.read_csv('/kaggle/input/corrected-context-ds/complete_context_dataset_corrected.csv')\ndf = train_df\ndf['context'] = df['context'].str.slice(0, 200)\ndf = df.drop(columns=['Unnamed: 0']).dropna().reset_index(drop=True)\n\ndf_val = pd.read_csv(\"/kaggle/input/60k-data-with-context-v2/train_with_context2.csv\")\ndf_val['context'] = df_val['context'].str.slice(0, 200)\n\ndata = Dataset.from_pandas(df)\ndata_val = Dataset.from_pandas(df_val)\n\ndef plot_sequence_lengths(data, max_length=1024): #filter abnormally long samples\n    sequence_lengths = []\n    keep_indices = []\n    for i, example in enumerate(data):\n        sequence_lengths.append(len(example['prompt']) + len(example['context']))\n        if sequence_lengths[i] < max_length:\n            keep_indices.append(i)\n    return keep_indices\n\nkeep_indices_train = plot_sequence_lengths(data)\ndata = data.select(keep_indices_train)\n\nkeep_indices_val = plot_sequence_lengths(data_val)\ndata_val = data_val.select(keep_indices_val)\ndf","metadata":{"execution":{"iopub.status.busy":"2023-10-11T00:21:22.449212Z","iopub.execute_input":"2023-10-11T00:21:22.449484Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def format_text(example):\n    text = prompt.format(prompt=example['prompt'], \n                         a=example['A'], \n                         b=example['B'], \n                         c=example['C'], \n                         d=example['D'], \n                         e=example['E'], \n                         context=example['context'])\n    return {\"text\": text}\n\ndata = data.map(format_text, num_proc = 56)\ndata_val = data_val.map(format_text, num_proc = 4)\ntokenizer = AutoTokenizer.from_pretrained(\"/kaggle/input/llama2-7b-hf/Llama2-7b-hf\", trust_remote_code=True)\ntokenizer.pad_token = tokenizer.eos_token","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(data['text'][0])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"FLAGS = {'MAX_INPUT': 64,\n         'LOGGING_STEPS': 10,\n         'NUM_EPOCHS': 2,\n         'BATCH_SIZE': 8,\n          'NUM_STEPS': len(data['text'])}","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def preprocess_function(example):\n    text_tokens = tokenizer(example[\"text\"], truncation=True, max_length=64, padding='max_length').input_ids\n    answer_tokens = tokenizer(example[\"answer\"], truncation=True, max_length=64, padding='max_length').input_ids\n    return {\n        \"input_ids\": text_tokens,\n        \"label\": answer_tokens,\n    }\n\ndata_train = data.map(preprocess_function, batched=False, num_proc=56).remove_columns(['prompt', 'context', 'A', 'B', 'C', 'D', 'E', 'answer', 'text']) #remove everything except for input_ids and labels","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!export XLA_USE_BF16=1\ndef train(index, FLAGS):\n    device = xm.xla_device()\n    train_sampler = torch.utils.data.distributed.DistributedSampler(data_train, num_replicas=8, rank=xm.get_ordinal(),\n                                                                    shuffle=True) #this guy is responsible for distributing data across 8 cores\n    training_loader = torch.utils.data.DataLoader(data_train, batch_size=FLAGS['BATCH_SIZE'], collate_fn=DataCollatorWithPadding(tokenizer=tokenizer),\n                                                  sampler=train_sampler)\n\n    xla_train_loader = pl.MpDeviceLoader(training_loader, device)\n    model = AutoModelForCausalLM.from_pretrained(\"/kaggle/input/llama2-7b-hf/Llama2-7b-hf\", torch_dtype=torch.bfloat16).to(device)\n    cnt = 0\n    for param in model.parameters(): #Freezing most of the layers\n        cnt += 1\n        param.requires_grad = True\n        if cnt < 285:\n            param.requires_grad = False\n\n    model.train()\n    num_replicas = 8\n\n    optimizer = torch.optim.Adam(model.parameters(), lr=1e-5)\n    scheduler = torch.optim.lr_scheduler.LinearLR(optimizer, start_factor=1.0, end_factor=0.1, total_iters=FLAGS['NUM_STEPS'] * FLAGS['BATCH_SIZE'])\n    print(f\"Initiliazed model and datasets on core {index}\")\n    num_iterations = int(FLAGS['NUM_STEPS'] / FLAGS['BATCH_SIZE'] / num_replicas)\n    for epoch in range(1, FLAGS['NUM_EPOCHS'] + 1):\n        for step, data in enumerate(xla_train_loader):\n            optimizer.zero_grad()\n            outputs = model(**data)\n            loss = outputs.loss\n            loss.backward()\n            xm.optimizer_step(optimizer)\n            if (step + 1) % FLAGS['LOGGING_STEPS'] == 0:\n                xm.master_print(f'Loss: {loss.item()}, {step + 1} steps out of {num_iterations}, LR: {optimizer.param_groups[0][\"lr\"]}')\n            scheduler.step()\n        xm.master_print(f\"Trained for {epoch} epochs out of {FLAGS['NUM_EPOCHS']}\")\n    xm.master_print(\"Waiting for all processes across cores to finish\")\n    xm.rendezvous('init')\n    xm.master_print(\"Saving the model\")\n    xm.save(model.state_dict(), \"tpu-llama.bin\")\nxmp.spawn(train, args=(FLAGS,), start_method='fork')","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}